{
  "name": "csvtojson",
  "description": "A tool concentrating on converting csv data to JSON with customised parser supporting",
  "author": {
    "name": "Keyang Xiang",
    "email": "keyang.xiang@gmail.com"
  },
  "homepage": "http://keyangxiang.com/blog/csv2json/",
  "bugs": {
    "url": "https://github.com/Keyang/node-csvtojson/issues"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/Keyang/node-csvtojson.git"
  },
  "contributors": [
    {
      "name": "Keyang Xiang",
      "email": "keyang.xiang@gmail.com"
    }
  ],
  "version": "0.3.19",
  "keywords": [
    "csv",
    "json",
    "convert",
    "parser",
    "exntendible",
    "plugin"
  ],
  "bin": {
    "csvtojson": "./bin/csvtojson"
  },
  "license": [
    {
      "type": "MIT",
      "url": "https://github.com/Keyang/node-csvtojson/blob/master/LICENSE"
    }
  ],
  "engines": {
    "node": ">=0.10"
  },
  "readme": "#CSV2JSON\nAll you need nodejs csv to json converter. Support big json data, CLI, web server, powerful nested JSON, customised parser, stream, pipe, and more!\n\n#IMPORTANT!!\nSince version 0.3, the core class of csvtojson has been inheriting from stream.Transform class. Therefore, it will behave like a normal Stream object and CSV features will not be available any more. Now the usage is like:\n```js\n//Converter Class\nvar Converter=require(\"csvtojson\").core.Converter;\nvar fs=require(\"fs\");\n\nvar csvFileName=\"./myCSVFile\";\nvar fileStream=fs.createReadStream(csvFileName);\n//new converter instance\nvar csvConverter=new Converter({constructResult:true});\n\n//end_parsed will be emitted once parsing finished\ncsvConverter.on(\"end_parsed\",function(jsonObj){\n   console.log(jsonObj); //here is your result json object\n});\n\n//read from file\nfileStream.pipe(csvConverter);\n```\n\nTo convert from a string, previously the code was:\n```js\ncsvConverter.from(csvString);\n```\n\nNow it is:\n```js\ncsvConverter.fromString(csvString,callback);\n```\n\nThe callback function above is optional. see [Parse String](#parse-string).\n\nAfter version 0.3, csvtojson requires node 0.10 and above.\n\n##Menu\n* [Installation](#installation)\n* [Example](#example)\n* [CLI Usage](#usage)\n    * [CLI Tool](#command-line-tools)\n    * [Web Service](#webservice)\n* [Demo Product](#demo-product)\n* [Quick Start](#quick-start)\n* [Parameters](#params)\n* [Customised Parser](#parser)\n* [Webserver](#webserver)\n* [Events](#events)\n* [Built-in Parsers](#default-parsers)\n* [Example](#example)\n* [Big CSV File Streaming](#big-csv-file)\n* [Process Big CSV File in CLI](#convert-big-csv-file-with-command-line-tool)\n* [Column Array](#column-array)\n* [Parse String](#parse-string)\n* [Empowered JSON Parser](#empowered-json-parser)\n* [Field Type](#field-type)\n* [Change Log](#change-log)\n\nGitHub: https://github.com/Keyang/node-csvtojson\n\n##Installation\n>npm install -g csvtojson\n\n\n##Features\n\n* Powerful library for you nodejs applications processing csv data.\n* Extremly straight forward\n* Multiple input support: CSV File, Readable Stream, CSV String etc.\n* Highly extendible with your own rules and parsers for outputs.\n* Multiple interfaces (webservice, command line)\n\n\n##Usage\n\n###Command Line Tools\n\n>csvtojson <csv file path>\n\nExample\n\n>csvtojson ./myCSVFile\n\nOr use pipe:\n\n>cat myCSVFile | csvtojson\n\nTo start a webserver\n\n>csvtojson startserver [options]\n\nAdvanced usage with parameters support, check help:\n\n>csvtojson --help\n\n### WebService\nAfter webserve being initialised, it is able to use http post with CSV data as body.\nFor example, we start web server with default configuration:\n>csvtojson startserver\n\nAnd then we use curl to perform a web request:\n>curl -X POST -d \"date,\\*json\\*employee.name,\\*json\\*employee.age,\\*json\\*employee.number,\\*array\\*address,\\*array\\*address,\\*jsonarray\\*employee.key,\\*jsonarray\\*employee.key,\\*omit\\*id\n>\n>2012-02-12,Eric,31,51234,Dunno Street,Kilkeny Road,key1,key2,2\n>\n>2012-03-06,Ted,28,51289,Cambridge Road,Tormore,key3,key4,4\" http://127.0.0.1:8801/parseCSV\n\n#Demo Product\nTo write a demo app, simply use csvtojson web interface. Paste following code to index.js:\n\n```js\nvar server=require(\"csvtojson\").interfaces.web;\n\nserver.startWebServer({\n\t\"port\":8801\n});\n```\nThen run the app:\n```\nnode ./index.js\n```\nNow you can post any csv data to http://localhost:8801/parseCSV\n\nIt uses HTTP Request as readable stream and HTTP Response as writable stream.\n\n# Quick Start\nUse csvtojson library to your own project.\n\nImport csvtojson to your package.json or install through npm:\n\n>npm install csvtojson\n\n~~The core of the tool is Converter class. It is based on node-csv library (version 0.3.6). Therefore it has all features of [node-csv](http://www.adaltas.com/projects/node-csv/).~~ To start a parse, simply use following code:\n\n```js\n//Converter Class\nvar Converter=require(\"csvtojson\").core.Converter;\nvar fs=require(\"fs\");\n\nvar csvFileName=\"./myCSVFile\";\nvar fileStream=fs.createReadStream(csvFileName);\n//new converter instance\nvar param={};\nvar csvConverter=new Converter(param);\n\n//end_parsed will be emitted once parsing finished\ncsvConverter.on(\"end_parsed\",function(jsonObj){\n   console.log(jsonObj); //here is your result json object\n});\n\n//read from file\nfileStream.pipe(csvConverter);\n```\n# Params\nThe parameters for Converter constructor are:\n\n* constructResult: true/false. Whether to constrcut final json object in memory which will be populated in \"end_parsed\" event. Set to false if deal with huge csv data. default: true.\n* delimiter: delimiter used for seperating columns. default: \",\"\n* quote: If a column contains delimiter, it is able to use quote character to surround the column content. e.g. \"hello, world\" wont be split into two columns while parsing. default: \" (double quote)\n* trim: Indicate if parser trim off spaces surrounding column content. e.g. \"  content  \" will be trimmed to \"content\". Default: true\n* checkType: This parameter turns on and off weather check field type. default is true. See [Field type](#field-type)\n* toArrayString: Stringify the stream output to JSON array. This is useful when pipe output to a file which expects JSON array. default is false and only JSON will be pushed to downstream.\n\n# Parser\nCSVTOJSON allows adding customised parsers which concentrating on what to parse and how to parse.\nIt is the main power of the tool that developer only needs to concentrate on how to deal with the data and other concerns like streaming, memory, web, cli etc are done automatically.\n\nHow to add a customised parser:\n\n```js\n//Parser Manager\nvar parserMgr=require(\"csvtojson\").core.parserMgr;\n\nparserMgr.addParser(\"myParserName\",/^\\*parserRegExp\\*/,function (params){\n   var columnTitle=params.head; //params.head be like: *parserRegExp*ColumnName;\n   var fieldName=columnTitle.replace(this.regExp, \"\"); //this.regExp is the regular expression above.\n   params.resultRow[fieldName]=\"Hello my parser\"+params.item;\n});\n```\n\nparserMgr's addParser function take three parameters:\n\n1. parser name: the name of your parser. It should be unique.\n\n2. Regular Expression: It is used to test if a column of CSV data is using this parser. In the example above any column's first row starting with *parserRegExp* will be using it.\n\n3. Parse function call back: It is where the parse happens. The converter works row by row and therefore the function will be called each time needs to parse a cell in CSV data.\n\nThe parameter of Parse function is a JSON object. It contains following fields:\n\n**head**: The column's first row's data. It generally contains field information. e.g. *array*items\n\n**item**: The data inside current cell.  e.g. item1\n\n**itemIndex**: the index of current cell of a row. e.g. 0\n\n**rawRow**: the reference of current row in array format. e.g. [\"item1\", 23 ,\"hello\"]\n\n**resultRow**: the reference of result row in JSON format. e.g. {\"name\":\"Joe\"}\n\n**rowIndex**: the index of current row in CSV data. start from 1 since 0 is the head. e.g. 1\n\n**resultObject**: the reference of result object in JSON format. It always has a field called csvRows which is in Array format. It changes as parsing going on. e.g.\n\n```json\n{\n   \"csvRows\":[\n      {\n          \"itemName\":\"item1\",\n          \"number\":10\n      },\n      {\n         \"itemName\":\"item2\",\n         \"number\":4\n      }\n   ]\n}\n```\n\n# WebServer\nIt is able to start the web server through code.\n\n```js\nvar webServer=require(\"csvtojson\").interfaces.web;\n\nvar server=webServer.startWebServer({\n   \"port\":\"8801\",\n   \"urlpath\":\"/parseCSV\"\n});\n```\n\n~~It will return an [expressjs](http://expressjs.com/) Application. You can add your own  web app content there.~~ It will return http.Server object.\n\n# Events\n\nFollowing events are used for Converter class:\n\n* end_parsed: It is emitted when parsing finished. the callback function will contain the JSON object if constructResult is set to true.\n* record_parsed: it is emitted each time a row has been parsed. The callback function has following parameters: result row JSON object reference, Original row array object reference, row index of current row in csv (header row does not count, first row content will start from 0)\n\nTo subscribe the event:\n\n```js\n//Converter Class\nvar Converter=require(\"csvtojson\").core.Converter;\n\n//end_parsed will be emitted once parsing finished\ncsvConverter.on(\"end_parsed\",function(jsonObj){\n    console.log(jsonObj); //here is your result json object\n});\n\n//record_parsed will be emitted each time a row has been parsed.\ncsvConverter.on(\"record_parsed\",function(resultRow,rawRow,rowIndex){\n   console.log(resultRow); //here is your result json object\n});\n```\n\n# Default Parsers\nThere are default parsers in the library they are\n\n~~**Array**: For columns head start with \"\\*array\\*\" e.g. \"\\*array\\*fieldName\", this parser will combine cells data with same fieldName to one Array.~~\n\n~~**Nested JSON**: For columns head start with \"\\*json\\*\" e.g. \"\\*json\\*my.nested.json.structure\", this parser will create nested nested JSON structure: my.nested.json~~\n\n~~**Nested JSON Array**: For columns head start with \"\\*jsonarray\\*\" e.g. \"\\*jsonarray\\*my.items\", this parser will create structure like my.items[].~~\n\n**JSON**: Any valid JSON structure (array, nested json) are supported. see [Empowered JSON Parser](#empowered-json-parser)\n\n**Omitted column**: For columns head start with \"\\*omit\\*\" e.g. \"\\*omit\\*id\", the parser will omit the column's data.\n\n#~~Example:~~(This example is deprecated see [Empowered JSON Parser](#empowered-json-parser))\n\nOriginal data:\n\n    date,*json*employee.name,*json*employee.age,*json*employee.number,*array*address,*array*address,*jsonarray*employee.key,*jsonarray*employee.key,*omit*id\n    2012-02-12,Eric,31,51234,Dunno Street,Kilkeny Road,key1,key2,2\n    2012-03-06,Ted,28,51289,Cambridge Road,Tormore,key3,key4,4\n\nOutput data:\n\n```json\n{\n   \"csvRows\": [\n      {\n         \"date\": \"2012-02-12\",\n         \"employee\": {\n            \"name\": \"Eric\",\n            \"age\": \"31\",\n            \"number\": \"51234\",\n            \"key\": [\n              \"key1\",\n              \"key2\"\n            ]\n          },\n          \"address\": [\n            \"Dunno Street\",\n            \"Kilkeny Road\"\n          ]\n        },\n        {\n          \"date\": \"2012-03-06\",\n          \"employee\": {\n            \"name\": \"Ted\",\n            \"age\": \"28\",\n           \"number\": \"51289\",\n            \"key\": [\n              \"key3\",\n              \"key4\"\n            ]\n         },\n         \"address\": [\n            \"Cambridge Road\",\n            \"Tormore\"\n         ]\n      }\n   ]\n}\n```\n\n# Big CSV File\ncsvtojson library was designed to accept big csv file converting. To avoid memory consumption, it is recommending to use read stream and write stream.\n\n```js\nvar Converter=require(\"csvtojson\").core.Converter;\nvar csvConverter=new Converter({constructResult:false}); // The parameter false will turn off final result construction. It can avoid huge memory consumption while parsing. The trade off is final result will not be populated to end_parsed event.\n\nvar readStream=require(\"fs\").createReadStream(\"inputData.csv\");\n\nvar writeStream=require(\"fs\").createWriteStream(\"outpuData.json\");\n\nreadStream.pipe(csvConverter).pipe(writeStream);\n```\n\nThe constructResult:false will tell the constructor not to combine the final result which would drain the memory as progressing. The output is piped directly to writeStream.\n\n# Convert Big CSV File with Command line tool\ncsvtojson command line tool supports streaming in big csv file and stream out json file.\n\nIt is very convenient to process any kind of big csv file. It's proved having no issue to proceed csv files over 3,000,000 lines (over 500MB) with memory usage under 30MB.\n\nOnce you have installed [csvtojson](#installation), you could use the tool with command:\n\n```\ncsvtojson [path to bigcsvdata] > converted.json\n```\n\nOr if you prefer streaming data in from another application:\n\n```\ncat [path to bigcsvdata] | csvtojson > converted.json\n```\n\nThey will do the same job.\n\n# Column Array\nTo convert a csv data to column array,  you have to construct the result in memory. See example below\n\n```js\nvar columArrData=__dirname+\"/data/columnArray\";\nvar rs=fs.createReadStream(columArrData);\nvar result = {}\nvar csvConverter=new CSVAdv();\n//end_parsed will be emitted once parsing finished\ncsvConverter.on(\"end_parsed\", function(jsonObj) {\n    console.log(result);\n    console.log(\"Finished parsing\");\n    done();\n});\n\n//record_parsed will be emitted each time a row has been parsed.\ncsvConverter.on(\"record_parsed\", function(resultRow, rawRow, rowIndex) {\n\n    for (var key in resultRow) {\n        if (!result[key] || !result[key] instanceof Array) {\n            result[key] = [];\n        }\n        result[key][rowIndex] = resultRow[key];\n    }\n\n});\nrs.pipe(csvConverter);\n```\n\nHere is an example:\n\n    TIMESTAMP,UPDATE,UID,BYTES SENT,BYTES RCVED\n    1395426422,n,10028,1213,5461\n    1395426422,n,10013,9954,13560\n    1395426422,n,10109,221391500,141836\n    1395426422,n,10007,53448,308549\n    1395426422,n,10022,15506,72125\n\nIt will be converted to:\n\n```json\n{\n  \"TIMESTAMP\": [\"1395426422\", \"1395426422\", \"1395426422\", \"1395426422\", \"1395426422\"],\n  \"UPDATE\": [\"n\", \"n\", \"n\", \"n\", \"n\"],\n  \"UID\": [\"10028\", \"10013\", \"10109\", \"10007\", \"10022\"],\n  \"BYTES SENT\": [\"1213\", \"9954\", \"221391500\", \"53448\", \"15506\"],\n  \"BYTES RCVED\": [\"5461\", \"13560\", \"141836\", \"308549\", \"72125\"]\n}\n```\n\n# Parse String\nTo parse a string, simply call fromString(csvString,callback) method. The callback parameter is optional.\n\nFor example:\n\n```js\nvar testData=__dirname+\"/data/testData\";\nvar data=fs.readFileSync(testData).toString();\nvar csvConverter=new CSVConverter();\n\n//end_parsed will be emitted once parsing finished\ncsvConverter.on(\"end_parsed\", function(jsonObj) {\n    //final result poped here as normal.\n});\ncsvConverter.fromString(data,function(err,jsonObj){\n    if (err){\n      //err handle\n    }\n    console.log(jsonObj);\n});\n\n```\n\n#Empowered JSON Parser\nSince version 0.3.8, csvtojson now can replicate any complex JSON structure.\nAs we know, JSON object represents a graph while CSV is only 2-dimension data structure (table).\nTo make JSON and CSV containing same amount information, we need \"flatten\" some information in JSON.\n\nHere is an example. Original CSV:\n\n```\nfieldA.title, fieldA.children[0].name, fieldA.children[0].id,fieldA.children[1].name, fieldA.children[1].employee[].name,fieldA.children[1].employee[].name, fieldA.address[],fieldA.address[], description\nFood Factory, Oscar, 0023, Tikka, Tim, Joe, 3 Lame Road, Grantstown, A fresh new food factory\nKindom Garden, Ceil, 54, Pillow, Amst, Tom, 24 Shaker Street, HelloTown, Awesome castle\n\n```\nThe data above contains nested JSON including nested array of JSON objects and plain texts.\n\nUsing csvtojson to convert, the result would be like:\n\n```json\n[{\n    \"fieldA\": {\n        \"title\": \"Food Factory\",\n        \"children\": [{\n            \"name\": \"Oscar\",\n            \"id\": \"0023\"\n        }, {\n            \"name\": \"Tikka\",\n            \"employee\": [{\n                \"name\": \"Tim\"\n            }, {\n                \"name\": \"Joe\"\n            }]\n        }],\n        \"address\": [\"3 Lame Road\", \"Grantstown\"]\n    },\n    \"description\": \"A fresh new food factory\"\n}, {\n    \"fieldA\": {\n        \"title\": \"Kindom Garden\",\n        \"children\": [{\n            \"name\": \"Ceil\",\n            \"id\": \"54\"\n        }, {\n            \"name\": \"Pillow\",\n            \"employee\": [{\n                \"name\": \"Amst\"\n            }, {\n                \"name\": \"Tom\"\n            }]\n        }],\n        \"address\": [\"24 Shaker Street\", \"HelloTown\"]\n    },\n    \"description\": \"Awesome castle\"\n}]\n```\n\nHere is the rule for CSV data headers:\n\n* Use dot(.) to represent nested JSON. e.g. field1.field2.field3 will be converted to {field1:{field2:{field3:< value >}}}\n* Use square brackets([]) to represent an Array. e.g. field1.field2[< index >] will be converted to {field1:{field2:[< values >]}}. Different column with same header name will be added to same array.\n* Array could contain nested JSON object. e.g. field1.field2[< index >].name will be converted to {field1:{field2:[{name:< value >}]}}\n* The index could be omitted in some situation. However it causes information lost. Therefore Index should **NOT** be omitted if array contains JSON objects with more than 1 field (See example above fieldA.children[1].employee field, it is still ok if child JSON contains only 1 field).\n\nSince 0.3.8, JSON parser is the default parser. It does not need to add \"\\*json\\*\" to column titles. Theoretically, the JSON parser now should have functionality of \"Array\" parser, \"JSONArray\" parser, and old \"JSON\" parser.\n\nThis mainly purposes on the next few versions where csvtojson could convert a JSON object back to CSV format without losing information.\nIt can be used to process JSON data exported from no-sql database like MongoDB.\n\n#Field Type\n\nFrom version 0.3.14, type of fields are supported by csvtojson.\nThe parameter checkType is used to whether to check and convert the field type.\nSee [here](#params) for the parameter usage.\n\nThank all who have contributed to ticket [#20](https://github.com/Keyang/node-csvtojson/issues/20).\n\n##Implict Type\n\nWhen checkType is turned on, parser will try to convert value to its implicit type if it is not explicitly specified.\n\nFor example, csv data:\n```csv\nname, age, married, msg\nTom, 12, false, {\"hello\":\"world\",\"total\":23}\n\n```\nWill be converted into:\n```json\n{\n  \"name\":\"Tom\",\n  \"age\":12,\n  \"married\":false,\n  \"msg\":{\n    \"hello\":\"world\",\n    \"total\":\"23\"\n  }\n}\n```\nIf checkType is turned **OFF**, it will be converted to:\n```json\n{\n  \"name\":\"Tom\",\n  \"age\":\"12\",\n  \"married\":\"false\",\n  \"msg\":\"{\\\"hello\\\":\\\"world\\\",\\\"total\\\":23}\"\n}\n```\n\n##Explicit Type\nCSV header column can explicitly define the type of the field.\nSimply add type before column name with a hash symbol (#).\n\n###Supported types:\n* string\n* number\n* date\n\n### Define Type\nTo define the field type, see following example\n```csv\nstring#appNumber, string#finished, date#startDate\n201401010002, true, 2014-01-01\n```\nThe data will be converted to:\n```json\n{\n  \"appNumber\":\"201401010002\",\n  \"finished\":\"true\",\n  \"startDate\":Wed Jan 01 2014 00:00:00 GMT+0000 (GMT)\n}\n```\n### Invalid Value\nIf parser meets invalid value for a type while parsing a value, it will fallback to use string value.\n\nFor example:\n```csv\nnumber#order, date#shipDate\nA00001, Unknown\n```\n\nIt will be converted to:\n```json\n{\n  \"order\":\"A00001\",\n  \"shipDate\":\"Unknown\"\n}\n```\n\n#Change Log\n\n##0.3.18\n* Fixed double qoute parse as per CSV standard.\n\n##0.3.14\n* Added field type support\n* Fixed some minor bugs\n\n##0.3.8\n* Empowered built-in JSON parser.\n* Change: Use JSON parser as default parser.\n* Added parameter trim in constructor. default: true. trim will trim content spaces.\n\n##0.3.5\n* Added fromString method to support direct string input\n\n##0.3.4\n* Added more parameters to command line tool.\n\n##0.3.2\n* Added quote in parameter to support quoted column content containing delimiters\n* Changed row index starting from 0 instead of 1 when populated from record_parsed event\n\n##0.3\n* Removed all dependencies\n* Deprecated applyWebServer\n* Added construct parameter for Converter Class\n* Converter Class now works as a proper stream object\n",
  "readmeFilename": "readme.md",
  "_id": "csvtojson@0.3.19",
  "dist": {
    "shasum": "99fe8c97936e6b42c815e4f7fe671032563b5f1d"
  },
  "_from": "csvtojson@",
  "_resolved": "https://registry.npmjs.org/csvtojson/-/csvtojson-0.3.19.tgz"
}
